---
title: "CS231n Lecture 8: 注意力机制与 Transformer"
description: "cs231n"
pubDate: "Feb 22 2026"
image: https://tc.alcy.cc/tc/20260121/0ff595905be4e17bbaedbb9224146ad7.webp
categories:
  - notebook
tags:
  - DL
  - Attention
  - Transformer
  - Sequence Modeling
---

## CS231n Lecture 8: 注意力机制与 Transformer

### 引言

在序列建模问题中，我们希望模型能够根据上下文理解当前元素的含义。循环神经网络通过在时间维度上递归更新隐藏状态来实现这一目标，但其内部状态始终是一个固定维度向量。当输入序列很长时，早期信息必须被不断压缩并通过多次非线性变换传递到后续时刻，这使得长距离依赖难以稳定学习。

一个更直接的思路是：在生成输出的每一步，不再依赖单一的压缩向量，而是直接“查看”整个输入序列，并根据当前需求选择性地聚合信息。注意力机制正是这一想法的数学化表达。它不再假设所有历史信息必须被压缩为单一状态，而是允许模型在任意时刻从整个序列中提取相关部分。

从抽象角度看，注意力是一种作用在向量集合上的算子：给定一个查询向量，它会在一组数据向量中计算相关性，并据此生成加权组合。Transformer 则是在网络的每一层都使用这种算子。

### 注意力的基本形式

设有一个查询向量 $q \in \mathbb{R}^d$，以及一组数据向量 $\{x_1, \dots, x_n\}$，每个 $x_i \in \mathbb{R}^d$。注意力机制的目标是根据 $q$ 与各个 $x_i$ 的相关程度生成一个新的表示。

首先计算相似度：

$$
e_i = q^\top x_i.
$$

为了避免高维空间中点积数值过大导致 softmax 饱和，通常进行缩放：

$$
e_i = \frac{q^\top x_i}{\sqrt{d}}.
$$

然后通过 softmax 将相似度转化为概率分布：

$$
a_i = \frac{\exp(e_i)}{\sum_j \exp(e_j)}.
$$

最终输出为加权和：

$$
y = \sum_i a_i x_i.
$$

直观理解是：查询向量决定“关注什么”，注意力权重刻画“关注多少”，输出则是基于重要性聚合后的结果。

### Query、Key 与 Value

在实际模型中，我们不会直接在原始向量上计算相似度，而是先通过线性变换生成三种表示：

$$
q = W_Q x, \quad k = W_K x, \quad v = W_V x.
$$

这里

- $q$ 表示查询（Query）
- $k$ 表示键（Key）
- $v$ 表示值（Value）

相似度在 $q$ 与 $k$ 之间计算，而输出是对 $v$ 的加权和。这种分离使模型能够在不同表示空间中分别学习“如何匹配”和“如何聚合”。

将单个查询扩展到矩阵形式。若输入为矩阵 $X \in \mathbb{R}^{n \times d}$，则有

$$
Q = XW_Q, \quad K = XW_K, \quad V = XW_V.
$$

注意力计算变为

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V.
$$

其中 $QK^\top$ 得到一个 $n \times n$ 的矩阵，表示每个位置对所有位置的关注程度。softmax 在行方向归一化，使每个位置形成一个对全序列的概率分布。

### 自注意力

当 $Q, K, V$ 都来自同一输入序列时，称为自注意力。此时，每个元素都会根据自身状态与其他所有元素的关系来更新表示：

$$
Q = XW_Q, \quad K = XW_K, \quad V = XW_V.
$$

新的表示为

$$
Y = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V.
$$

每个输出向量 $y_i$ 都是所有输入向量的加权组合，因此其表示包含全局上下文信息。

自注意力具有一个重要性质：若对输入序列进行相同的排列，输出也会按相同方式排列。这意味着自注意力本质上作用在“集合”而非“有序序列”上，因此本身不包含顺序信息。

### 位置编码

由于自注意力无法感知顺序，必须人为注入位置信息。常见做法是为每个位置添加一个位置向量 $p_i$：

$$
\tilde{x}_i = x_i + p_i.
$$

位置编码可以是固定函数（如正弦余弦），也可以是可学习参数。它为模型提供关于“第几个元素”的信号，使其能够区分不同位置。

### 掩码机制

在自回归生成任务中，模型在预测第 $t$ 个位置时不能看到未来信息。可以在相似度矩阵中将未来位置的值替换为 $-\infty$，使 softmax 后对应权重为零。这种机制称为 masked self-attention。

### 多头注意力

单个注意力层在一个表示子空间中建模关系，但序列中的依赖可能具有多种类型。多头注意力通过并行运行多个注意力层，使模型在不同子空间中学习不同关系。

设有 $H$ 个头，每个头维度为 $d_h = d / H$。第 $i$ 个头计算：

$$
\text{head}_i =\text{Attention}(QW_i^Q, KW_i^K, VW_i^V).
$$

然后将所有头的输出拼接并线性映射：

$$
\text{MultiHead}(Q, K, V) =
\text{Concat}(\text{head}_1, \dots, \text{head}_H) W_O.
$$

这种结构使模型能够同时捕获不同类型的依赖关系。

### Transformer 的基本结构

Transformer 完全基于注意力机制构建。一个标准的 Transformer 块包含两部分：多头自注意力层和前馈网络层，每一部分都带有残差连接与归一化：

$$
z_1 = \text{LayerNorm}(x + \text{MultiHead}(x)),
$$

$$
z_2 = \text{LayerNorm}(z_1 + \text{FFN}(z_1)).
$$

前馈网络通常为两层线性变换与非线性激活：

$$
\text{FFN}(x)=\max(0, xW_1 + b_1)W_2 + b_2.
$$

通过多层堆叠，模型逐渐将局部表示转化为高层语义表示。

### 编码器与解码器

编码器由若干自注意力与前馈层堆叠而成，负责生成上下文表示。解码器结构类似，但在自注意力中加入掩码以防止看到未来信息，并包含一个交叉注意力层，用于从编码器输出中提取相关信息。

交叉注意力的形式为：

$$
\text{Attention}(Q_{\text{dec}}, K_{\text{enc}}, V_{\text{enc}}).
$$

这种结构使模型在生成序列时既能利用已生成内容，又能动态访问输入。

### 复杂度与优势

自注意力的时间复杂度为 $O(n^2)$，因为需要计算所有位置对之间的关系。尽管复杂度较高，但其优势在于完全并行计算和全局感受野。与 RNN 相比，它不再依赖顺序递归传播信息，因此训练更加高效，长距离依赖也更容易建模。

### 总结

注意力机制提供了一种新的序列建模方式：通过查询与加权求和在整个序列中动态选择相关信息。自注意力使每个元素都能与其他所有元素交互，从而获得全局上下文。Transformer 在此基础上构建，通过多层堆叠的注意力与前馈网络实现强大的表示能力。

理解注意力作为“在向量集合上执行加权聚合”的本质，是理解现代序列模型的关键一步。
