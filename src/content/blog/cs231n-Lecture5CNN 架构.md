---
title: "CS231n Lecture 6: CNN 架构、训练技巧与迁移学习"
description: "cs231n"
pubDate: "Feb 12 2026"
image: https://tc.alcy.cc/tc/20260121/c0d548529c912fdfacbf4fc7179b7f94.webp
categories:
  - notebook
tags:
  - CV
  - CNN
  - Transfer Learning
---

## CS231n Lecture 6: CNN 架构、训练技巧与迁移学习

[课程链接](https://cs231n.stanford.edu/)

### 引言

构建一个成功的卷积神经网络不仅需要理解其基本构件，还需掌握一系列工程实践与设计哲学。本讲系统性地探讨了如何**构建**和**训练**CNN，涵盖了从权重初始化、激活函数选择到经典架构（如 VGG 和 ResNet）的演进，并深入分析了数据预处理、正则化、超参数调优以及迁移学习等关键训练策略。

### 激活函数：非线性的引擎

![](https://github.com/lnscq/picx-images-hosting/raw/master/image.86u69uoo0l.webp)

激活函数为网络引入非线性，是其具备强大表示能力的根本。Sigmoid 函数 $\sigma(x) = 1/(1+e^{-x})$ 因其在饱和区梯度趋近于零，导致深层网络训练困难。**ReLU（Rectified Linear Unit）** $f(x) = \max(0, x)$ 因其计算简单、在正区梯度恒为1，成为事实标准，显著加速了训练收敛。尽管存在“神经元死亡”的风险，但其在实践中的优越性使其被广泛采用。更平滑的变体如 **GELU（Gaussian Error Linear Unit）** $f(x) = x \Phi(x)$（$\Phi$ 为标准正态累积分布函数）在某些场景下表现更佳，但计算成本更高。

### 经典 CNN 架构的演进

CNN 架构的发展史是一部对深度、效率与表示能力的探索史。

#### VGGNet

![](https://github.com/lnscq/picx-images-hosting/raw/master/image.4jommbubsm.webp)
VGGNet 的核心洞见是：**使用小尺寸卷积核（$3\times3$）堆叠可以替代大尺寸卷积核，同时获得更深的网络和更强的非线性**。三个连续的 $3\times3$ 卷积层具有与单个 $7\times7$ 卷积层相同的感受野（receptive field），但前者拥有更多的非线性激活（三次 ReLU vs. 一次），且参数量更少（$3 \times (3^2 C^2) = 27C^2$ vs. $7^2 C^2 = 49C^2$）。这一设计使得 VGGNet 能够构建 16 或 19 层的深度模型，在 ImageNet 上取得了显著成功。

然而，简单地堆叠更多层会导致**退化问题（Degradation Problem）**：更深的“普通”网络（plain network）反而比浅层网络具有更高的训练误差。这并非由过拟合引起，而是优化困难所致——随着层数增加，梯度在反向传播中逐渐消失或爆炸，使得深层难以有效训练。

#### ResNet（残差网络）

![](https://github.com/lnscq/picx-images-hosting/raw/master/image.8hh0306f0j.webp)
ResNet 通过引入**残差连接（skip connection）** 巧妙地解决了这一问题。其核心思想是让网络学习一个**残差映射** $F(\mathbf{x})$，而非直接学习所需的底层映射 $H(\mathbf{x})$。一个残差块的输出定义为：

$$
\mathbf{y} = F(\mathbf{x}) + \mathbf{x},
$$

其中 $F(\mathbf{x})$ 是由若干卷积层组成的子网络。这种结构使得网络在理想情况下只需将 $F(\mathbf{x})$ 学习为零，即可实现恒等映射（identity mapping），从而保证深层网络至少不会比其浅层版本表现更差。这一设计极大地缓解了梯度消失问题，使得训练上百甚至上千层的网络成为可能，引领了“深度革命”。

### 权重初始化：稳定训练的起点

不恰当的权重初始化会破坏信号在网络中的流动。若初始权重过小，前向传播的激活值会逐层衰减至零；若过大，则激活值会迅速爆炸。**Kaiming/MSRA 初始化**为此提供了理论指导。对于使用 ReLU 激活的网络，它建议从均值为0、标准差为 $\sqrt{2 / D_{\text{in}}}$ 的高斯分布中采样权重，其中 $D_{\text{in}}$ 是输入维度。该方案能有效维持各层激活值的方差大致恒定，为稳定、高效的训练奠定了基础。

### 数据预处理与增强：提升泛化能力

**数据预处理**是标准化输入的关键步骤。标准做法是对每个颜色通道分别计算训练集的均值 $\mu_c$ 和标准差 $\sigma_c$，然后对所有图像执行：

$$
X_{:, c, :, :} \leftarrow \frac{X_{:, c, :, :} - \mu_c}{\sigma_c}.
$$

这使得输入数据以零为中心并具有单位方差，有利于优化器工作。

**数据增强（Data Augmentation）** 是一种强大的正则化技术，通过对训练图像施加随机变换来人为扩充数据集。常用方法包括：

- **水平翻转**：适用于大多数物体类别。
- **随机裁剪与缩放**：如 ResNet 中采用的多尺度训练策略，在 $[256, 480]$ 像素间随机选择短边长度，再裁剪出固定大小（如 $224\times224$）的区域。
- **色彩抖动**：随机调整亮度、对比度、饱和度等。

这些变换在训练时引入了随机性，在测试时则通过平均多个增强版本的预测结果（如 10-crop averaging）来消除随机性，从而提升模型鲁棒性。

### 正则化与迁移学习

![](https://github.com/lnscq/picx-images-hosting/raw/master/image.7axoueiuu0.webp)

**Dropout** 是另一种重要的正则化手段。在训练时，它以概率 $p$（通常为 0.5）随机将神经元的输出置零，迫使网络学习冗余的特征表示，防止神经元间的共适应。在测试时，所有神经元都参与计算，但其输出需乘以 $p$ 以保持期望一致。

当目标任务数据稀缺时，**迁移学习（Transfer Learning）** 提供了高效解决方案。其标准流程分为两步：

1.  **预训练**：在一个大型源数据集（如 ImageNet）上训练一个通用 CNN。
2.  **微调（Fine-tuning）**：将预训练模型作为起点，在目标数据集上进行进一步训练。

具体策略取决于目标数据集的规模与相似度：

- 对于**小而相似**的数据集，通常冻结预训练的卷积主干（backbone），仅重新训练顶部的全连接分类层。
- 对于**大而不同**的数据集，则可使用预训练权重作为良好的初始化，对整个网络进行端到端微调。

现代深度学习框架提供的“模型动物园”（Model Zoo）极大简化了这一过程，使研究者能快速利用在海量数据上预训练的强大特征提取器。

### 超参数调优的实用指南

高效的超参数搜索遵循一套系统化流程：

1.  **验证初始损失**：确保损失值在合理范围内。
2.  **过拟合小样本**：用极小数据集（如 10 张图）验证模型容量和实现正确性。
3.  **寻找有效学习率**：在完整数据集上尝试不同学习率（如 $10^{-1}$ 到 $10^{-5}$），选择能使损失在百次迭代内显著下降的值。
4.  **粗粒度网格搜索**：在选定的学习率附近，对其他超参数（如 batch size, weight decay）进行粗略搜索。
5.  **精炼与监控**：基于粗搜结果进行精细搜索，并通过观察训练/验证损失与准确率曲线判断模型状态（欠拟合、过拟合或正在收敛）。

值得注意的是，**随机搜索**通常比网格搜索更高效，因为它能更充分地探索重要超参数的取值空间。

### 总结

本讲全面覆盖了构建和训练现代 CNN 所需的核心知识。从 VGG 的小卷积核堆叠到 ResNet 的残差学习，架构设计不断突破深度的极限；从 Kaiming 初始化到数据增强，训练技巧确保了模型的稳定与泛化；而迁移学习则将大规模预训练的知识迁移到下游任务，成为小数据场景下的利器。这些原则与实践共同构成了深度视觉系统开发的坚实基础。
