---
title: "CS231n Lecture 2: 图像分类与线性分类器"
description: cs231n
pubDate: "Feb 8 2026"
image: https://tc.alcy.cc/tc/20260121/d3f9e567dd3e998ba9862561ebb6de93.webp
categories:
  - notebook
tags:
  - CV
  - Image Classification
  - Linear Classifier
---

## CS231n Lecture 2: 图像分类与线性分类器

[课程链接](https://cs231n.stanford.edu/)

### 引言

图像分类是计算机视觉的基础任务，其目标是为输入图像分配一个预定义的语义类别标签。尽管对人类而言该任务近乎直觉，但对计算机而言却极具挑战：图像在底层仅表示为高维像素张量（如 $800 \times 600 \times 3$ 的 RGB 值），而语义信息需从中抽象得出。这一从原始像素到高层语义的映射困难，被称为“语义鸿沟”。

### 图像分类的挑战

实际场景中，同一类别的视觉表现存在巨大变异。典型因素包括视角变化、光照条件、背景杂乱、遮挡、非刚性形变、类内差异以及上下文依赖。这些因素导致相同语义内容在像素空间中可能相距甚远，使得基于手工规则或像素级相似性的方法难以奏效。

### 数据驱动范式

为应对上述挑战，现代方法采用数据驱动的学习范式：给定一个带标签的数据集 $\mathcal{D} = \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^N$，算法从数据中自动学习映射函数 $f: \mathbf{x} \mapsto y$。该范式摒弃了硬编码规则，转而依赖大量标注样本以归纳泛化能力。

### K近邻分类器

K近邻（k-Nearest Neighbors, kNN）是最简单的非参数分类器。其训练阶段仅存储全部训练样本；预测时，对测试样本 $\mathbf{x}$，计算其与所有训练样本的距离，选取最近的 $K$ 个邻居，并通过多数投票决定类别。

常用距离度量包括 L1 距离：

$$
d_1(\mathbf{x}, \mathbf{x}') = \sum_i |x_i - x'_i|,
$$

和 L2 距离：

$$
d_2(\mathbf{x}, \mathbf{x}') = \sqrt{\sum_i (x_i - x'_i)^2}.
$$

尽管概念直观，kNN 存在显著缺陷：推理复杂度为 $O(N)$，难以扩展；在高维空间（如 CIFAR-10 的 3072 维）中受“维度灾难”影响，距离度量失去判别性；更重要的是，像素级距离对语义无关的扰动（如平移、微小形变）极为敏感，无法反映真实语义相似性。因此，kNN 主要用于教学目的，而非实际系统。

### 超参数选择与验证集

kNN 的性能依赖超参数 $K$ 和距离度量的选择。为避免在训练集上过拟合（如 $K=1$ 时训练准确率恒为 100%）或污染测试集，标准做法是将原始训练集划分为训练子集与验证子集。超参数在验证集上进行调优，最终模型在独立测试集上评估。对于小规模数据，可采用交叉验证以获得更稳健的估计。

### 线性分类器

为克服 kNN 的局限，参数化模型被引入。线性分类器通过可学习参数实现高效泛化。设输入图像 $\mathbf{x} \in \mathbb{R}^D$（通常展平为向量），权重矩阵 $\mathbf{W} \in \mathbb{R}^{C \times D}$，偏置 $\mathbf{b} \in \mathbb{R}^C$，则分类得分由下式给出：

$$
f(\mathbf{x}; \mathbf{W}, \mathbf{b}) = \mathbf{W} \mathbf{x} + \mathbf{b}.
$$

每一行 $\mathbf{w}_j^\top$ 可视为类别 $j$ 的模板，分类即计算输入与各模板的点积相似度，并选择得分最高的类别。

该模型将整个训练集的知识压缩至 $C \times D$ 个参数中，推理仅需一次矩阵乘法，效率极高。然而，其决策边界为线性超平面，无法处理线性不可分问题（如异或分布），限制了表达能力。尽管如此，线性分类器构成了后续非线性模型（如神经网络）的基础模块。

### 损失函数与优化目标

为学习参数 $\mathbf{W}$ 和 $\mathbf{b}$，需定义损失函数以量化预测误差。常见选择包括支持向量机（SVM）的合页损失与 Softmax 分类器的交叉熵损失。学习过程转化为如下优化问题：

$$
\min_{\mathbf{W}, \mathbf{b}} \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f(\mathbf{x}^{(i)}; \mathbf{W}, \mathbf{b}), y^{(i)}) + \lambda R(\mathbf{W}),
$$

其中 $R(\cdot)$ 为正则项，$\lambda$ 控制正则强度。该优化框架是深度学习的核心机制。
