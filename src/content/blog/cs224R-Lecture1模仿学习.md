---
title: "cs224R-Lecture1 Imitation Learning"
description: "cs224R"
pubDate: "Feb 6 2026"
image: https://tc.alcy.cc/tc/20260121/5414bd765cac05c430529647e839d8cd.webp
categories:
  - notebook
tags:
  - RL
  - Imitation Learning
---
## Lecture 1 Imitation Learning
### 0.引言
在上一讲中，我们将强化学习形式化为一个清晰的优化问题：通过与环境的交互，学习一个策略，使其在期望意义下最大化累计奖励。从理论角度看，这一表述是完备且优雅的。然而，在真实系统中，这一范式往往直接遭遇实践层面的困难。

首先，环境交互的成本在许多任务中是不可忽略的。真实机器人、自动驾驶系统或面向用户的在线系统，都无法承受大规模、无约束的试错过程。其次，奖励函数在许多任务中并不容易被精确定义。人类通常能够判断“一个行为是否合理”，但将这种判断转化为稳定、可优化的标量信号本身就是一个困难的问题。

更进一步，在不少场景中，人类专家已经能够完成任务，并且已经积累了大量可供记录的行为数据。这自然引出了一个核心问题：在不显式构造奖励函数、也不依赖大规模环境交互的前提下，是否可以直接从专家行为中学习策略？

因此，我们引入了模仿学习（Imitation Learning）。

### 1.基本问题设定

模仿学习假设我们可以访问一组由专家策略生成的示范数据。这些数据通常以轨迹的形式给出，每条轨迹由状态序列及其对应的专家动作构成。形式化地，可以将数据集表示为
$$
\mathcal{D} = {(s_1, a_1, \dots, s_T, a_T)}
$$

其中数据由某个未知但性能良好的专家策略 $\pi_{\text{expert}}$ 采样得到。

模仿学习的目标并不是推断环境动力学或奖励函数，而是直接学习一个参数化策略  $\pi_\theta$，使其在状态分布上尽可能复现专家的决策行为。从这一角度看，模仿学习的关注点不在于“什么是好的状态”，而在于“专家在这些状态下采取了什么样的行动”。

这一设定在形式上与监督学习高度相似，但其后果却截然不同。

### 2.行为克隆

行为克隆（Behavior Cloning, BC）的最常见形式，是用监督学习拟合专家动作。若采用确定性策略，典型目标为平方损失回归：
$$
\min_\theta \mathbb{E}_{(s,a) \sim D} \left[ \|a - \hat a\|^2 \right], \hat a = \pi_\theta (s)
$$

这一写法在形式上简洁，但它隐含了一个非常强的建模偏置：在给定状态 $s$ 时，模型被鼓励输出一个“单值”动作 $\hat a$ 。当专家行为在同一状态下近似唯一时，这种偏置并不明显；然而在实际数据中，专家往往并非单一来源，或存在等价可行的多种决策模式，于是问题就会显现。

### 为什么 $L_2$ 回归会学到“均值”？

![](https://lnscq.github.io/picx-images-hosting/image.9gx355jie0.webp)

如果示范数据在某些状态上呈现多模态（例如一部分专家选择“并入左侧车道”，另一部分选择“保持/并入右侧”），那么用 $L_2$ 损失训练的确定性政策会倾向输出**条件均值**。更具体的说，在固定 $s$ 的条件下，最小化 $\mathbb{E} \|a - \hat a\|^2$ 的最优解满足：
$$
\hat a^*(s) = \mathbb{E}[a \mid s]
$$

这意味着：当数据分布在动作空间有两个峰时，均值往往落在“两个峰之间”的低概率区域。在驾驶这样的安全敏感任务里，这种折中动作会是灾难性的。

因此，我们可以认识到：模仿学习的挑战不一定来自网络不够大，而可能来自输出分布的表达能力不足。换句话说，“函数逼近能力”与“分布表达能力”是两件不同的事。